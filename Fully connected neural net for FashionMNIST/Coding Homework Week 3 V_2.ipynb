{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784,300)\n",
    "        self.fc2 = nn.Linear(300,100)\n",
    "        self.fc3 = nn.Linear(100,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "#         print(x.shape)\n",
    "        x = x.view(10,-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) \n",
    "#         print(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train(mode = True)\n",
    "    stasiscounter =0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         print(output.shape)\n",
    "#         print(target.shape)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = 10\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.train(mode=False)\n",
    "    model.eval()\n",
    "    confusion_matrix=torch.zeros(no_classes,no_classes)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i ,(data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            for t,p in zip(target.view(-1),pred.view(-1)):\n",
    "                confusion_matrix[t.long(),p.long()] +=1\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, confusion_matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(confusion_matrix):\n",
    "    return (confusion_matrix.diag()/confusion_matrix.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndsplit_simple(x,y):\n",
    "    \n",
    "    numtr = x.shape[0]//2\n",
    "    \n",
    "    inds=np.arange(y.size)\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    xtr=x[inds[0:numtr],:]\n",
    "    ytr=y[inds[0:numtr]]\n",
    "\n",
    "    xv=x[inds[numtr:],:]\n",
    "    yv=y[inds[numtr:]]\n",
    "\n",
    "    return xtr,ytr,xv,yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 10\n",
    "    test_batch_size = 10\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.001\n",
    "    log_interval=1000\n",
    "    save_model = False\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    kwargs = {}\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    \n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    best_confusion_matrix = []\n",
    "    least_loss = 10000 #best loss\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss.append(train(log_interval, model, device, train_loader, optimizer, epoch))\n",
    "        test_loss, confusion_matrix, accuracy = test(log_interval, model, device, test_loader)\n",
    "        val_loss.append(test_loss)\n",
    "        if test_loss < least_loss:\n",
    "            least_loss = test_loss\n",
    "            best_epoch=epoch\n",
    "            best_confusion_matrix = confusion_matrix\n",
    "            best_accuracy = accuracy\n",
    "    if (save_model):\n",
    "        torch.save(model.state_dict(),\"FashionMnist_cnn.pt\")\n",
    "        \n",
    "#     per_class_accuracy = per_class_accuracy(best_confusion_matrix)\n",
    "    return best_confusion_matrix, least_loss,best_epoch,train_loss,val_loss, best_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.367893\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.909662\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.449381\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.222664\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.434014\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.047858\n",
      "\n",
      "Test set: Average loss: 0.1541, Accuracy: 9527/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.018771\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.386369\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.179325\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.036005\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.045144\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.130765\n",
      "\n",
      "Test set: Average loss: 0.1081, Accuracy: 9659/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.052547\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.051515\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.108504\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.042664\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.516997\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.129788\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.207440\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.041585\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.001051\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.021182\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.136653\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.002451\n",
      "\n",
      "Test set: Average loss: 0.0777, Accuracy: 9743/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000474\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.066219\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.052942\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.002343\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.302070\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.002459\n",
      "\n",
      "Test set: Average loss: 0.0730, Accuracy: 9753/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.002350\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.033426\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.101641\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.000305\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.002547\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.006307\n",
      "\n",
      "Test set: Average loss: 0.0664, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.008967\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.000945\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.003665\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.000493\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.000288\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.057464\n",
      "\n",
      "Test set: Average loss: 0.0639, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000683\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.001190\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.002506\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.000819\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.007975\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.070061\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 9798/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000251\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.006950\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.011797\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.011719\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.044763\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.000248\n",
      "\n",
      "Test set: Average loss: 0.0664, Accuracy: 9796/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000842\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.000528\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.007526\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.007987\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.002214\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.038947\n",
      "\n",
      "Test set: Average loss: 0.0644, Accuracy: 9809/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_confusion_matrix,least_loss,best_epoch,train_losses, valid_losses,best_accuracy = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(valid_losses)\n",
    "\n",
    "plt.title('model losses')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training loss', 'validation loss with test set'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch: 7\n",
      "least loss = 0.06391231496334077\n",
      "best accuracy obtained as an average over all classes = 97.97\n",
      "class 0 : 0.9877551198005676\n",
      "class 1 : 0.9903083443641663\n",
      "class 2 : 0.9825581312179565\n",
      "class 3 : 0.9831683039665222\n",
      "class 4 : 0.9806517362594604\n",
      "class 5 : 0.9809417128562927\n",
      "class 6 : 0.9739039540290833\n",
      "class 7 : 0.9678988456726074\n",
      "class 8 : 0.9733059406280518\n",
      "class 9 : 0.9752230048179626\n",
      "\n",
      "Sorted classwise accuracies\n",
      "class 1: 0.9903083443641663\n",
      "class 0: 0.9877551198005676\n",
      "class 3: 0.9831683039665222\n",
      "class 2: 0.9825581312179565\n",
      "class 5: 0.9809417128562927\n",
      "class 4: 0.9806517362594604\n",
      "class 9: 0.9752230048179626\n",
      "class 6: 0.9739039540290833\n",
      "class 8: 0.9733059406280518\n",
      "class 7: 0.9678988456726074\n",
      "Hardest class to predict: 7 with accuracy 0.9678988456726074\n"
     ]
    }
   ],
   "source": [
    "print(\"best epoch: \" + str(best_epoch))\n",
    "print(\"least loss = \" + str(least_loss))\n",
    "print(\"best accuracy obtained as an average over all classes = \" + str(best_accuracy))\n",
    "class_accuracy = per_class_accuracy(best_confusion_matrix)\n",
    "for target,accuracy in enumerate(class_accuracy):\n",
    "    print('class ' + str(target)+ ' : ' + str(accuracy.item()))\n",
    "\n",
    "# mapped = map (lambda k: (k,class_accuracy[k]),class_accuracy)\n",
    "# for idx in mapped:\n",
    "#     print(idx)\n",
    "accuracy_rank = sorted(range(len(class_accuracy)), key=lambda k: -class_accuracy[k])\n",
    "print()\n",
    "print(\"Sorted classwise accuracies\")\n",
    "for idx in accuracy_rank:\n",
    "    print(\"class \" + str(idx) + ': ' + str(class_accuracy[idx].item()) )\n",
    "# print(\"ranking of class by classwise-accuracy is\", accuracy_rank)\n",
    "#     print(accuracy.item())\n",
    "# print(\"classwise accuracy: \" , per_class_accuracy(best_confusion_matrix))\n",
    "print('Hardest class to predict: {} with accuracy {}'.format(\n",
    "                accuracy_rank[-1], class_accuracy[accuracy_rank[-1]].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
