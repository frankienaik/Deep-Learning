{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "#from sets import Set\n",
    "def parsesynsetwords(filen):\n",
    "\n",
    "  synsetstoclassdescriptions={}\n",
    "  indicestosynsets={}\n",
    "  synsetstoindices={}\n",
    "  ct=-1\n",
    "  with open(filen) as f:\n",
    "    for line in f:\n",
    "      if (len(line)> 5):\n",
    "        z=line.strip().split()\n",
    "        descr=''\n",
    "        for i in range(1,len(z)):\n",
    "          descr=descr+' '+z[i]\n",
    "        \n",
    "        ct+=1\n",
    "        indicestosynsets[ct]=z[0]\n",
    "        synsetstoindices[z[0]]=ct\n",
    "        synsetstoclassdescriptions[z[0]]=descr[1:]\n",
    "  return indicestosynsets,synsetstoindices,synsetstoclassdescriptions\n",
    "\n",
    "\n",
    "def test_parsesyn():\n",
    "  filen = 'synset_words.txt'\n",
    "  indicestosynsets,synsetstoindices,synsetstoclassdescr=parsesynsetwords(filen)\n",
    "  clsdict=get_classes()\n",
    "  \n",
    "  '''\n",
    "  for keyval in indicestosynsets.items():\n",
    "    print(type(keyval[0]),keyval[0],keyval[1])\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  \n",
    "  for i in range(1000):\n",
    "    n1=synsetstoclassdescr[indicestosynsets[i]]\n",
    "    n2=clsdict[i]\n",
    "    \n",
    "    if(n1!=n2):\n",
    "      print (i)\n",
    "      print ('n1', n1,'n2:',n2 )\n",
    "\n",
    "def testparse():\n",
    "  nm='ILSVRC2012_bbox_val_v3/val/ILSVRC2012_val_00049999.xml'\n",
    "\n",
    "  tree = ET.parse(nm)\n",
    "  root = tree.getroot()\n",
    "  \n",
    "  '''\n",
    "  for child in root:\n",
    "    #print child.tag,'|'\n",
    "    if child.tag=='object':\n",
    "      for el in child:\n",
    "        if el.tag=='name':\n",
    "          print el.text\n",
    "  '''        \n",
    "  for obj in root.findall('object'):\n",
    "     for name in obj.findall('name'):\n",
    "       print (name.text)\n",
    "  #for         \n",
    "  \n",
    "def parseclasslabel(nm,synsetstoindices):  \n",
    "  tree = ET.parse(nm)\n",
    "  root = tree.getroot()\n",
    "\n",
    "  lbset=set()\n",
    "  \n",
    "  for obj in root.findall('object'):\n",
    "     for name in obj.findall('name'):\n",
    "       #print name.text\n",
    "       ind=synsetstoindices[name.text]\n",
    "       firstname=name.text\n",
    "       lbset.add(ind)\n",
    "       \n",
    "  if len(lbset)!=1:\n",
    "    print     ('ERR: len(lbset)!=1',  len(lbset))\n",
    "    exit()\n",
    "    \n",
    "  for s in lbset:\n",
    "    label=  s\n",
    "  return label,firstname\n",
    "  \n",
    "  \n",
    "def test_parseclasslabel():\n",
    "  filen='synset_words.txt'\n",
    "\n",
    "  \n",
    "  nm='ILSVRC2012_bbox_val_v3/val/ILSVRC2012_val_00049999.xml'\n",
    "  \n",
    "  indicestosynsets,synsetstoindices,synsetstoclassdescr=parsesynsetwords(filen)\n",
    "  \n",
    "  label,firstname=parseclasslabel(nm,synsetstoindices)\n",
    "  \n",
    "  print(label,firstname,  synsetstoclassdescr[indicestosynsets[label]] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   \\n    def getitem(self):\\n            \\n        filen=self.path_sysnet + 'synset_words.txt'\\n        for num in range(1,2501):\\n            nm= self.path_labels + 'ILSVRC2012_val_'+str(num).zfill(8) +'.xml'\\n            img_name = self.path_image + 'ILSVRC2012_val_'+str(num).zfill(8)+'.jpeg' \\n            \\n            indicestosynsets,synsetstoindices,synsetstoclassdescr=gic.parsesynsetwords(filen)\\n            image = io.imread(img_name)\\n            \\n            label,firstname=gic.parseclasslabel(nm,synsetstoindices)\\n            self.sysnet_list.append((label,firstname,  synsetstoclassdescr[indicestosynsets[label]] ))\\n            self.image_list.append(image)\\n            \\n            return self.sysnet_list,self.image_list\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getimagenetclassesNewV2 as gic\n",
    "from skimage import data, io, filters\n",
    "import cv2\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import skimage.color\n",
    "import cv2\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "\n",
    "class imagenetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,path_sysnet,path_image,path_labels,transform = None):\n",
    "        self.path_sysnet = path_sysnet\n",
    "        self.path_labels = path_labels\n",
    "        self.path_image = path_image\n",
    "        self.sysnet_list =[]\n",
    "        self.image_list=[]\n",
    "        self.transformations = transform\n",
    "        self.rgbtransform = transforms.Compose(transforms.Grayscale(3))\n",
    "#         self.transformations = transforms.Compose([transforms.Resize(224),\n",
    "#                                                   transforms.CenterCrop(224),\n",
    "#                                                   transforms.Grayscale(1),\n",
    "#                                                    transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2500\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        filen=self.path_sysnet + 'synset_words.txt'\n",
    "        nm= self.path_labels + 'ILSVRC2012_val_'+str(index+1).zfill(8)+'.xml'\n",
    "        img_name = self.path_image + 'ILSVRC2012_val_'+str(index+1).zfill(8)+'.jpeg' \n",
    "        \n",
    "        indicestosynsets,synsetstoindices,synsetstoclassdescr=gic.parsesynsetwords(filen)\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "#         image = image.convert('L')\n",
    "#         image = cv2.cvtColor(cv2.UMat(image), cv2.COLOR_BGR2GRAY)\n",
    "#         print(image.size)\n",
    "#         print(image.ndim)\n",
    "#         Image.convert('RGB',image)\n",
    "#         print(Image.getchannel('R'))\n",
    "#         print(image.size)\n",
    "#         if(image.shape[0] == 1):\n",
    "#             image = self.rgbtransform(image)\n",
    "        \n",
    "\n",
    "#         print(image.shape)\n",
    "\n",
    "#             img = image.convert('LA')\n",
    "#             image = cv2.cvtColor(cv2.UMat(image), cv2.COLOR_BGR2GRAY)\n",
    "#             skimage.color.rgb2gray(image)\n",
    "#             image = rgb2gray(image)\n",
    "\n",
    "        \n",
    "#         image = self.transformations(img)\n",
    "        \n",
    "#         print(img_name)\n",
    "#         if (image.shape[0]!=1):\n",
    "#             print(image.shape)\n",
    "        label,firstname=gic.parseclasslabel(nm,synsetstoindices)\n",
    "#         self.sysnet_list.append((label,firstname,  synsetstoclassdescr[indicestosynsets[label]] ))\n",
    "#         self.image_list.append(image)\n",
    "        \n",
    "        labels = (label,firstname,  synsetstoclassdescr[indicestosynsets[label]])\n",
    "        \n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        \n",
    "#         print(labels.shape)\n",
    "#         result = {'image':image,'labels':(label,firstname,  synsetstoclassdescr[indicestosynsets[label]])}\n",
    "        \n",
    "#         return result\n",
    "#         print('next')\n",
    "#         print(image)\n",
    "#         if (torch.all(torch.eq(image[0], image[1]))):\n",
    "#             print(img_name)\n",
    "        return image,labels\n",
    "\n",
    "'''   \n",
    "    def getitem(self):\n",
    "            \n",
    "        filen=self.path_sysnet + 'synset_words.txt'\n",
    "        for num in range(1,2501):\n",
    "            nm= self.path_labels + 'ILSVRC2012_val_'+str(num).zfill(8) +'.xml'\n",
    "            img_name = self.path_image + 'ILSVRC2012_val_'+str(num).zfill(8)+'.jpeg' \n",
    "            \n",
    "            indicestosynsets,synsetstoindices,synsetstoclassdescr=gic.parsesynsetwords(filen)\n",
    "            image = io.imread(img_name)\n",
    "            \n",
    "            label,firstname=gic.parseclasslabel(nm,synsetstoindices)\n",
    "            self.sysnet_list.append((label,firstname,  synsetstoclassdescr[indicestosynsets[label]] ))\n",
    "            self.image_list.append(image)\n",
    "            \n",
    "            return self.sysnet_list,self.image_list\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = 1000\n",
    "def test(model, device, test_loader):\n",
    "    model.train(mode=False)\n",
    "    model.eval()\n",
    "    confusion_matrix=torch.zeros(no_classes,no_classes)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i ,(data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)#, target.to(device)\n",
    "            labels = target[0].to(device)\n",
    "            \n",
    "            if (len(data.shape) == 5): #if it is fivecrop\n",
    "                bs, ncrops, c, h, w = data.size()\n",
    "                data = data.view(-1,c,h,w)\n",
    "                output = model(data)\n",
    "                output = output.view(bs,ncrops,-1).mean(1)\n",
    "#             labels = torch.unsqueeze(labels,1).shape\n",
    "            else:\n",
    "                output = model(data)\n",
    "    \n",
    "            \n",
    "#             print(output)\n",
    "#             print(data.shape)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "#             print(pred.shape)\n",
    "#             print(torch.unsqueeze(labels,1).shape)\n",
    "            \n",
    "            for t,p in zip(labels.view(-1),pred.view(-1)):\n",
    "                confusion_matrix[t.long(),p.long()] +=1\n",
    "            \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "         correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return confusion_matrix, accuracy\n",
    "\n",
    "\n",
    "class Squeezenet_330(nn.Module):\n",
    "    def __init__(self, num_classes=no_classes):\n",
    "        super(Squeezenet_330,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d((224,224))\n",
    "        self.trained_model = models.squeezenet1_1(pretrained = True)\n",
    "    def forward(self,x):\n",
    "        x = self.avgPool(x)\n",
    "        x = self.trained_model.features(x)\n",
    "        x = self.trained_model.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)\n",
    "    \n",
    "class Inceptionnet_330(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=no_classes):\n",
    "        super(Inceptionnet_330, self).__init__()\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d((224,224))\n",
    "        self.trained_model = models.inception_v3(pretrained=True)\n",
    "    def forward(self, x):\n",
    "        x = self.avgPool(x)\n",
    "        x = self.trained_model.forward(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "D:\\Anaconda3\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1:\n",
      "\n",
      "Test set: Accuracy: 414/2500 (17%)\n",
      "\n",
      "Accuracy for unnormalized data:  16.56\n",
      "\n",
      "Test set: Accuracy: 1442/2500 (58%)\n",
      "\n",
      "Accuracy for normalized data:  57.68\n",
      "--------------------------------------------------\n",
      "Problem 2\n",
      "\n",
      "Test set: Accuracy: 1517/2500 (61%)\n",
      "\n",
      "Accuracy for FiveCrop data:  60.68\n",
      "--------------------------------------------------\n",
      "Problem 3:\n",
      "\n",
      "Test set: Accuracy: 1454/2500 (58%)\n",
      "\n",
      "Accuracy for FiveCrop data (squeezenet):  58.16\n",
      "\n",
      "Test set: Accuracy: 1785/2500 (71%)\n",
      "\n",
      "Accuracy for FiveCrop data (inception):  71.4\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "def main():\n",
    "    sysnet_path = ''\n",
    "    image_path = 'imagenet2500/imagespart/'\n",
    "    label_path = 'ILSVRC2012_bbox_val_v3/val/'\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "#     resnet18 = vision.resnet18_v1(pretrained=True)\n",
    "#     squeezenet = vision.squeezenet1_0(pretrained = True)\n",
    "    inception = models.inception_v3(pretrained=True)\n",
    "    squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "    \n",
    "    \n",
    "    print(\"Problem 1:\")\n",
    "    #Problem 1:\n",
    "    image_datasetQ1_norm = imagenetDataset(sysnet_path,image_path,label_path,transforms.Compose([transforms.Resize(224),\n",
    "                                                      transforms.CenterCrop(224),\n",
    "    #                                                         transforms.Grayscale(1),\n",
    "                                                        transforms.ToTensor(),   \n",
    "                                                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "                                   )\n",
    "    image_datasetQ1 = imagenetDataset(sysnet_path,image_path,label_path,transforms.Compose([transforms.Resize(224),\n",
    "                                                      transforms.CenterCrop(224),\n",
    "    #                                                         transforms.Grayscale(3),\n",
    "                                                            transforms.ToTensor()]  )\n",
    "                                     )\n",
    "\n",
    "#     print(image_datasetQ1.__getitem__(0))\n",
    "#     print(image_datasetQ1_norm.__getitem__(0))\n",
    "    \n",
    "    image_dataset_loaderQ1_norm = torch.utils.data.DataLoader(dataset=image_datasetQ1_norm,\n",
    "                                                        batch_size=10,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    image_dataset_loaderQ1 = torch.utils.data.DataLoader(dataset=image_datasetQ1,\n",
    "                                                        batch_size=10,\n",
    "                                                        shuffle=False)\n",
    "    \n",
    "    #not normalized\n",
    "    _,accuracyQ1 = test(squeezenet,device,image_dataset_loaderQ1)\n",
    "    #normalized\n",
    "    \n",
    "    print(\"Accuracy for unnormalized data: \", accuracyQ1)\n",
    "    \n",
    "    _,accuracyQ1_norm = test(squeezenet,device,image_dataset_loaderQ1_norm)\n",
    "#     for data , target in image_dataset_loaderQ1:\n",
    "#         print(data)\n",
    "#         break\n",
    "#     for data , target in image_dataset_loaderQ1_norm:\n",
    "#         print(data)\n",
    "#         break\n",
    "    print(\"Accuracy for normalized data: \", accuracyQ1_norm)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Problem 2\")\n",
    "    # problem 2:\n",
    "    image_datasetQ2 = imagenetDataset(\n",
    "        sysnet_path,image_path,label_path,transforms.Compose(\n",
    "            [transforms.Resize(280),\n",
    "            transforms.FiveCrop(224),\n",
    "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "            transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(norm) for norm in norms]))])\n",
    "\n",
    "#             (lambda crops: torch.stack([transforms.Compose([transforms.ToTensor(), \n",
    "#                                                             transforms.Normalize(\n",
    "#                                                                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "#                                                             )])(crop) for crop in crops]))]  \n",
    "        )\n",
    "    \n",
    "    image_dataset_loaderQ2 = torch.utils.data.DataLoader(dataset=image_datasetQ2,\n",
    "                                                        batch_size=10,\n",
    "                                                        shuffle=False)    \n",
    "\n",
    "    _,accuracyQ2 = test(squeezenet,device,image_dataset_loaderQ2)\n",
    "                        \n",
    "    print(\"Accuracy for FiveCrop data: \", accuracyQ2)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(\"Problem 3:\")\n",
    "    # problem 3:\n",
    "    image_datasetQ3 = imagenetDataset(\n",
    "        sysnet_path,image_path,label_path,transforms.Compose(\n",
    "            [transforms.Resize(330),\n",
    "            transforms.FiveCrop(330),\n",
    "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "            transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(norm) for norm in norms]))])\n",
    "\n",
    "#             (lambda crops: torch.stack([transforms.Compose([transforms.ToTensor(), \n",
    "#                                                             transforms.Normalize(\n",
    "#                                                                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "#                                                             )])(crop) for crop in crops]))]  \n",
    "        )\n",
    "    \n",
    "    image_dataset_loaderQ3 = torch.utils.data.DataLoader(dataset=image_datasetQ3,\n",
    "                                                        batch_size=10,\n",
    "                                                        shuffle=False)    \n",
    "    \n",
    "\n",
    "    _,accuracyQ3 = test(Squeezenet_330(),device,image_dataset_loaderQ3)\n",
    "    \n",
    "    print(\"Accuracy for FiveCrop data (squeezenet): \", accuracyQ3)\n",
    "\n",
    "    _,accuracyQ3_2 = test(Inceptionnet_330(),device,image_dataset_loaderQ3)\n",
    "    \n",
    "    print(\"Accuracy for FiveCrop data (inception): \", accuracyQ3_2)\n",
    "#     print(\"Accuracy for FiveCrop data: \", accuracyQ2)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
